---
title: "Assignment 5 - Unusual Plot"
author: "Matthew Thompson"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, results = 'hide', message = FALSE, warning = FALSE)

# Author: Matthew Thompson

# File Name: [Assignment 5].R
# Date: 2025-02-20

# Description: unusual plots

# Purpose: [Brief description of the script's objective]

# Setting up the sourcing file directory
snippets_dir <- "/Users/matthewthompson/Documents/R Programming/Comprehensive Snippets/Functions"

# Packages and loaded (choose to source or manually install and load yourself)
source(file.path(snippets_dir, "install_data_science_packages.R"))

#manually install (if applicable) and load libraries below:
#type here for libraries and packages

# Install Addins (optional)
source(file.path(snippets_dir, "install_addins.R"))

# Source the R version functions
source(file.path(snippets_dir, "print_r_version.R"))

# Version:
print_r_version()

#Project Directory Overview:
source(file.path(snippets_dir, "project_directory_tree.R"))
project_directory_tree()

# Change Log:
#2025-02-20: Initial version

```

```{r assessment, echo =FALSE}
##########
#Importing data
##########
dangerous_job <- read.csv("Dangerous Jobs.csv")

##########
#Assessment
##########
head(dangerous_job) 
summary(dangerous_job)

#seems there are 4515 NA values in NAICS and 
#93,146 NA values in Fatalities
#I think this data was mentioned to have a lot of dupes so

```

```{r data quality, echo =FALSE}
##########
#Data Quality and Cleaning
##########
#first things first
#I want to check for unique values in case

# data <- dangerous_job 
# data2 <- data %>% 
#   filter(!is.na(MajorGroup) & !is.na(Cause) & !is.na(Fatalities) & Cause != "N/A") %>%
#   group_by (MajorGroup, Cause) %>%
#   summarise(TotalFatalities = sum(Fatalities, na.rm = TRUE), .groups = "drop")
# 
# data3 <- data2 %>%
#   filter(Cause != "Total.Fatalities") %>%
#   group_by(MajorGroup, Cause) %>%
#   summarise(TotalFatalities = sum(TotalFatalities), .groups = "drop") %>%
#   arrange(desc(TotalFatalities))
# 
# data4 <- data3 %>%
#   filter(MajorGroup %in% c("Heavy And Civil Engineering", 
#                            "Justice, Public Order, and Safety Activities", 
#                            "Administrative And Support Services", 
#                            "Truck Transportation", 
#                            "Specialty Trade Contractors"))

#duplicated(dangerous_job) #not efficient
duplicated_dangerous_job <- dangerous_job %>%
  group_by_all() %>%
  filter(n() > 1) %>%
  ungroup()
#seems like a lot of dupes

removed_dupes <- dangerous_job %>%
  group_by_all() %>%
  distinct()
#removed dupes across ALL columns 
#helps to have unique values across all columns 

removed_dupes2.0 <- dangerous_job %>%
  distinct(across())
#just verifying if this works the same way
#yep, it works 

#still have sneaky dupes in Fatalities column 
#for example, in detailed occupation like pigs
#and hoglins, their fatalities are the same
# and same year. Only thing different is NAICS
#I do not know if NAICS being different is correct or not
#despite same group and occupation but Ill try to remove
#in case

removed_dupes0.1 <- dangerous_job %>% #wanted to try a lil different approach
  group_by(MajorGroup, MinorGroup, 
           BroadOccupation, DetailedOccupation, 
           Cause) %>%
  #avoids distinct() effect on year, NAICS, and Fatalities 
  distinct(Fatalities, .keep_all = TRUE) 
#seems annoying dupes are finally removed!
#can verify it works since fatalities column
#still have same numbers but I can't find dupes
#across groups and occupation columns

#now, I need to investigate if NA values are either intentionally 0 
#or if they are truly missing, or simply unknown
filtered_NA <- removed_dupes0.1 %>%
  filter(is.na(NAICS) | is.na(Fatalities))
#it's likely  NA in groups and occupations are unknown
#rather than missing but hard to tell

filtered_NA2 <- removed_dupes0.1 %>%
  filter(is.na(NAICS) & 
           is.na(Fatalities & 
                   is.na(MajorGroup) & 
                   is.na(MinorGroup) & 
                   is.na(BroadOccupation) & 
                   is.na(DetailedOccupation) & 
                   is.na(Cause)))
#wanted to see if any industries have NA values
#which could suggest truly missing data on
#fatalities
#no results

#I think I can safely assume NA values are unknown
#rather than being truly missing
#I checked and usually groups and occupations are
#missing where only detailed occupation is known
#possibly suggesting a very niche field 
#where it may not accurately fit? 

```

```{r data transformation, echo =FALSE}

##########
# Data transformation
##########
#let's try to replace NA in these groups to UNKNOWN
#then Fatalities NA with 0 

data_prep <- removed_dupes0.1 

replaced_NA <- data_prep %>%
  mutate(NAICS = as.character(NAICS)) %>%
  mutate(NAICS = ifelse(is.na(NAICS), "UNKNOWN", NAICS),
         MajorGroup = ifelse(is.na(MajorGroup), "UNKNOWN", MajorGroup),
         MinorGroup = ifelse(is.na(MinorGroup), "UNKNOWN", MinorGroup),
         BroadOccupation = ifelse(is.na(BroadOccupation), "UNKNOWN", BroadOccupation),
         Fatalities = ifelse(is.na(Fatalities), 0, Fatalities))

#Now my question I want to answer will decide
#how I transform the data

#possible questions:
#what are the most common fatalities across 
#top 10 most dangerous jobs? 

#how does dangerous jobs change over time 
#and what fatalities look like? (Stream graph?)

#seems stream graph is likely the most
#appropriate for showing high-number data
#over time

grouped_data <- replaced_NA %>%
  dplyr::select(Year, MajorGroup, Fatalities) %>%
  filter(Cause == "Total.Fatalities") %>%
  group_by(MajorGroup, Year) %>% 
  summarize(Fatalities = sum(Fatalities)) %>% 
  arrange(desc(Fatalities))
#most top 10 dangerous jobs come from 
#unknown groups so I may need to use
#detailed occupation instead

grouped_data2 <- replaced_NA %>%
  dplyr::select(Year, DetailedOccupation, Fatalities) %>%
  filter(Cause == "Total.Fatalities" & DetailedOccupation != "Total") %>%
  group_by(DetailedOccupation, Year) %>% 
  summarize(Fatalities = sum(Fatalities)) %>% 
  arrange(desc(Fatalities))

#I need to transform it using dcast to show years across 

grouped_data3 <- grouped_data2 %>%
  group_by(Year) %>%
  top_n(5, wt = Fatalities) 
#too many groups so cut down to 5 

dcast_data <- grouped_data3 %>%
  dcast(DetailedOccupation ~ Year, value.var = "Fatalities", fill = 0)
#obtained top 10 dangerous jobs across years
# Detailed Occupations to ensure no unknown groups
#based on total fatalities 

```
\
\
```{r plot stream, echo =FALSE, results='show'}

##########
#Plotting with Stream Graph
##########

library(ggstream)

# col = c("#D6E6F8",
#         "#B4D9FA", 
#         "#93BFF0", 
#         "#77A9E0", 
#         "#5E98D6", 
#         "#3681C7", 
#         "#1C6BB7", 
#         "#034B9E", 
#         "#00266B", 
#         "black")

col = c("#D6E6F8",
        "#B4D9FA", 
        "#93BFF0", 
        "#77A9E0", 
        "red", 
        "#3681C7", 
        "#1C6BB7", 
        "#034B9E", 
        "#00266B", 
        "black")

p1 <- ggplot(grouped_data3, aes(x = Year, y = Fatalities, 
                          group = DetailedOccupation, 
                          fill = DetailedOccupation)) +
  geom_stream(type = "proportional", stat = "identity", alpha = 1) +
  theme_minimal() +
  labs(x = "Year",
       y = "Proportional Fatalities",
       fill = "Occupation") +
  scale_fill_manual(values = col) +
  #scale_fill_brewer(palette = "Blues") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p2 <- ggplot(grouped_data3, aes(x = Year, y = Fatalities,
                          group = DetailedOccupation,
                          fill = DetailedOccupation)) +
  geom_stream(stat = "identity", alpha = 1) +
  theme_minimal() +
  labs(x = "Year",
       y = "Total Fatalities",
       fill = "Occupation") +
  scale_fill_manual(values = col) +
  #scale_fill_brewer(palette = "Blues") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
#dang facet_wrap does not work with stream graphs

library(patchwork)

combined_plots <- (p1/p2) +
  plot_layout(ncol = 1, guides = "collect")
combined_plots

#proportional is bit easier to read with proportions
#but does not show total counts 

#default setting is harder to read
#but shows total counts over time

```

**Figure 1.** How does top 10 dangerous jobs and their fatalities change over time? The stream graph shows the top 5 dangerous jobs across years by total fatalities which is proportional (Top). In around 2017 & 2018, the total fatalities were the highest (Bottom). The private industry was showcased in red showing the consistency of higher fatality rates in around 2019-2022 compared to other jobs (Bottom). Note that negative values do not actually reflect the total counts, but rather how the data was stacked symmetrically (Bottom). Proportional (Top) and Total Fatalities (Bottom) graphs are shown altogether to compare and also to better understand the stream graphs.

Note: Colors were adjusted to try to accommodate colorblinds by relying on varying shades of blues and a contrasting red.

\newpage 
## R Entire Script

```{r entire code1, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggstream)
library(patchwork)

##########
#Importing data
##########
dangerous_job <- read.csv("Dangerous Jobs.csv")

##########
#Assessment
##########
head(dangerous_job) 
summary(dangerous_job)

#seems there are 4515 NA values in NAICS and 
#93,146 NA values in Fatalities
#I think this data was mentioned to have a lot of dupes so

##########
#Data Quality and Cleaning
##########
#first things first
#I want to check for unique values in case

#duplicated(dangerous_job) #not efficient
duplicated_dangerous_job <- dangerous_job %>%
  group_by_all() %>%
  filter(n() > 1) %>%
  ungroup()
#seems like a lot of dupes

removed_dupes <- dangerous_job %>%
  group_by_all() %>%
  distinct()
#removed dupes across ALL columns 
#helps to have unique values across all columns 

removed_dupes2.0 <- dangerous_job %>%
  distinct(across())
#just verifying if this works the same way
#yep, it works 

```
\newpage
```{r entirecode2, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
#still have sneaky dupes in Fatalities column 
#for example, in detailed occupation like pigs
#and hoglins, their fatalities are the same
# and same year. Only thing different is NAICS
#I do not know if NAICS being different is correct or not
#despite same group and occupation but Ill try to remove
#in case

removed_dupes0.1 <- dangerous_job %>% #wanted to try a lil different approach
  group_by(MajorGroup, MinorGroup, 
           BroadOccupation, DetailedOccupation, 
           Cause) %>%
  #avoids distinct() effect on year, NAICS, and Fatalities 
  distinct(Fatalities, .keep_all = TRUE) 
#seems annoying dupes are finally removed!
#can verify it works since fatalities column
#still have same numbers but I can't find dupes
#across groups and occupation columns

#now, I need to investigate if NA values are either intentionally 0 
#or if they are truly missing, or simply unknown
filtered_NA <- removed_dupes0.1 %>%
  filter(is.na(NAICS) | is.na(Fatalities))
#it's likely  NA in groups and occupations are unknown
#rather than missing but hard to tell

filtered_NA2 <- removed_dupes0.1 %>%
  filter(is.na(NAICS) & 
           is.na(Fatalities & 
                   is.na(MajorGroup) & 
                   is.na(MinorGroup) & 
                   is.na(BroadOccupation) & 
                   is.na(DetailedOccupation) & 
                   is.na(Cause)))
#wanted to see if any industries have NA values
#which could suggest truly missing data on
#fatalities
#no results

#I think I can safely assume NA values are unknown
#rather than being truly missing
#I checked and usually groups and occupations are
#missing where only detailed occupation is known
#possibly suggesting a very niche field 
#where it may not accurately fit? 

```
\newpage
```{r entirecode3, echo =TRUE, results='hide', fig.show='hide', message=FALSE, warning=FALSE}
##########
# Data transformation
##########
#let's try to replace NA in these groups to UNKNOWN
#then Fatalities NA with 0 

data_prep <- removed_dupes0.1 

replaced_NA <- data_prep %>%
  mutate(NAICS = as.character(NAICS)) %>%
  mutate(NAICS = ifelse(is.na(NAICS), "UNKNOWN", NAICS),
         MajorGroup = ifelse(is.na(MajorGroup), "UNKNOWN", MajorGroup),
         MinorGroup = ifelse(is.na(MinorGroup), "UNKNOWN", MinorGroup),
         BroadOccupation = ifelse(is.na(BroadOccupation), "UNKNOWN", BroadOccupation),
         Fatalities = ifelse(is.na(Fatalities), 0, Fatalities))

#Now my question I want to answer will decide
#how I transform the data

#possible questions:
#what are the most common fatalities across 
#top 10 most dangerous jobs? 

#how does dangerous jobs change over time 
#and what fatalities look like? (Stream graph?)

#seems stream graph is likely the most
#appropriate for showing high-number data
#over time

grouped_data <- replaced_NA %>%
  dplyr::select(Year, MajorGroup, Fatalities) %>%
  filter(Cause == "Total.Fatalities") %>%
  group_by(MajorGroup, Year) %>% 
  summarize(Fatalities = sum(Fatalities)) %>% 
  arrange(desc(Fatalities))
#most top 10 dangerous jobs come from 
#unknown groups so I may need to use
#detailed occupation instead

grouped_data2 <- replaced_NA %>%
  dplyr::select(Year, DetailedOccupation, Fatalities) %>%
  filter(Cause == "Total.Fatalities" & DetailedOccupation != "Total") %>%
  group_by(DetailedOccupation, Year) %>% 
  summarize(Fatalities = sum(Fatalities)) %>% 
  arrange(desc(Fatalities))

#I need to transform it using dcast to show years across 

grouped_data3 <- grouped_data2 %>%
  group_by(Year) %>%
  top_n(5, wt = Fatalities) 
#too many groups so cut down to 5 

dcast_data <- grouped_data3 %>%
  dcast(DetailedOccupation ~ Year, value.var = "Fatalities", fill = 0)
#obtained top 10 dangerous jobs across years
# Detailed Occupations to ensure no unknown groups
#based on total fatalities 

##########
#Plotting with Stream Graph
##########

library(ggstream)

# col = c("#D6E6F8",
#         "#B4D9FA", 
#         "#93BFF0", 
#         "#77A9E0", 
#         "#5E98D6", 
#         "#3681C7", 
#         "#1C6BB7", 
#         "#034B9E", 
#         "#00266B", 
#         "black")

col = c("#D6E6F8",
        "#B4D9FA", 
        "#93BFF0", 
        "#77A9E0", 
        "red", 
        "#3681C7", 
        "#1C6BB7", 
        "#034B9E", 
        "#00266B", 
        "black")

p1 <- ggplot(grouped_data3, aes(x = Year, y = Fatalities, 
                          group = DetailedOccupation, 
                          fill = DetailedOccupation)) +
  geom_stream(type = "proportional", stat = "identity", alpha = 1) +
  theme_minimal() +
  labs(x = "Year",
       y = "Proportional Fatalities",
       fill = "Occupation") +
  scale_fill_manual(values = col) +
  #scale_fill_brewer(palette = "Blues") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p2 <- ggplot(grouped_data3, aes(x = Year, y = Fatalities,
                          group = DetailedOccupation,
                          fill = DetailedOccupation)) +
  geom_stream(stat = "identity", alpha = 1) +
  theme_minimal() +
  labs(x = "Year",
       y = "Total Fatalities",
       fill = "Occupation") +
  scale_fill_manual(values = col) +
  #scale_fill_brewer(palette = "Blues") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
#dang facet_wrap does not work with stream graphs

library(patchwork)

combined_plots <- (p1/p2) +
  plot_layout(ncol = 1, guides = "collect")
combined_plots

#proportional is bit easier to read with proportions
#but does not show total counts 

#default setting is harder to read
#but shows total counts over time

```